{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Praveena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download Brown corpus if not already downloaded\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown Corpus Sections: ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "# 1. Explore the sections (categories) in the Brown Corpus\n",
    "sections = brown.categories()\n",
    "print(\"Brown Corpus Sections:\", sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of documents in each section:\n",
      "adventure: 29\n",
      "belles_lettres: 75\n",
      "editorial: 27\n",
      "fiction: 29\n",
      "government: 30\n",
      "hobbies: 36\n",
      "humor: 9\n",
      "learned: 80\n",
      "lore: 48\n",
      "mystery: 24\n",
      "news: 44\n",
      "religion: 17\n",
      "reviews: 17\n",
      "romance: 29\n",
      "science_fiction: 6\n"
     ]
    }
   ],
   "source": [
    "# 2. Categorize documents in each section\n",
    "section_documents = {section: len(brown.fileids(categories=section)) for section in sections}\n",
    "print(\"\\nNumber of documents in each section:\")\n",
    "for section, count in section_documents.items():\n",
    "    print(f\"{section}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count is 100554\n",
      "Sentence count is 4623\n",
      "No. of Genres: {'news'}\n"
     ]
    }
   ],
   "source": [
    "# 3. Choose a section and analyze it (e.g., 'news')\n",
    "section_name = 'news'\n",
    "file_ids = brown.fileids(categories=section_name)\n",
    "# Count the number of words in the chosen section\n",
    "word_count = sum(len(brown.words(fileid)) for fileid in file_ids)\n",
    "print(\"Word count is\",word_count)\n",
    "# Count the number of sentences in the chosen section\n",
    "sentence_count = sum(len(brown.sents(fileid)) for fileid in file_ids)\n",
    "print(\"Sentence count is\",sentence_count)\n",
    "# Find all genres in the chosen section\n",
    "unique_genres = set(brown.categories(fileids=file_ids))\n",
    "print(\"No. of Genres:\",unique_genres)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'Wh' word counts in 'news' section:\n",
      "which: 245\n",
      "when: 169\n",
      "who: 268\n",
      "whether: 18\n",
      "where: 59\n",
      "what: 95\n",
      "why: 14\n",
      "whom: 8\n",
      "how: 42\n",
      "whose: 22\n"
     ]
    }
   ],
   "source": [
    "# 4. Count 'wh' words in the chosen section\n",
    "wh_words = ['what', 'why', 'who', 'whom', 'where', 'when', 'which', 'whose', 'whether', 'how']\n",
    "wh_word_count = Counter(word.lower() for fileid in file_ids for word in brown.words(fileid) if word.lower() in wh_words)\n",
    "print(f\"\\n'Wh' word counts in '{section_name}' section:\")\n",
    "for wh_word, count in wh_word_count.items():\n",
    "    print(f\"{wh_word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File IDs:\n",
      "['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', 'ca18', 'ca19', 'ca20', 'ca21', 'ca22', 'ca23', 'ca24', 'ca25', 'ca26', 'ca27', 'ca28', 'ca29', 'ca30', 'ca31', 'ca32', 'ca33', 'ca34', 'ca35', 'ca36', 'ca37', 'ca38', 'ca39', 'ca40', 'ca41', 'ca42', 'ca43', 'ca44', 'cb01', 'cb02', 'cb03', 'cb04', 'cb05', 'cb06', 'cb07', 'cb08', 'cb09', 'cb10', 'cb11', 'cb12', 'cb13', 'cb14', 'cb15', 'cb16', 'cb17', 'cb18', 'cb19', 'cb20', 'cb21', 'cb22', 'cb23', 'cb24', 'cb25', 'cb26', 'cb27', 'cc01', 'cc02', 'cc03', 'cc04', 'cc05', 'cc06', 'cc07', 'cc08', 'cc09', 'cc10', 'cc11', 'cc12', 'cc13', 'cc14', 'cc15', 'cc16', 'cc17', 'cd01', 'cd02', 'cd03', 'cd04', 'cd05', 'cd06', 'cd07', 'cd08', 'cd09', 'cd10', 'cd11', 'cd12', 'cd13', 'cd14', 'cd15', 'cd16', 'cd17', 'ce01', 'ce02', 'ce03', 'ce04', 'ce05', 'ce06', 'ce07', 'ce08', 'ce09', 'ce10', 'ce11', 'ce12', 'ce13', 'ce14', 'ce15', 'ce16', 'ce17', 'ce18', 'ce19', 'ce20', 'ce21', 'ce22', 'ce23', 'ce24', 'ce25', 'ce26', 'ce27', 'ce28', 'ce29', 'ce30', 'ce31', 'ce32', 'ce33', 'ce34', 'ce35', 'ce36', 'cf01', 'cf02', 'cf03', 'cf04', 'cf05', 'cf06', 'cf07', 'cf08', 'cf09', 'cf10', 'cf11', 'cf12', 'cf13', 'cf14', 'cf15', 'cf16', 'cf17', 'cf18', 'cf19', 'cf20', 'cf21', 'cf22', 'cf23', 'cf24', 'cf25', 'cf26', 'cf27', 'cf28', 'cf29', 'cf30', 'cf31', 'cf32', 'cf33', 'cf34', 'cf35', 'cf36', 'cf37', 'cf38', 'cf39', 'cf40', 'cf41', 'cf42', 'cf43', 'cf44', 'cf45', 'cf46', 'cf47', 'cf48', 'cg01', 'cg02', 'cg03', 'cg04', 'cg05', 'cg06', 'cg07', 'cg08', 'cg09', 'cg10', 'cg11', 'cg12', 'cg13', 'cg14', 'cg15', 'cg16', 'cg17', 'cg18', 'cg19', 'cg20', 'cg21', 'cg22', 'cg23', 'cg24', 'cg25', 'cg26', 'cg27', 'cg28', 'cg29', 'cg30', 'cg31', 'cg32', 'cg33', 'cg34', 'cg35', 'cg36', 'cg37', 'cg38', 'cg39', 'cg40', 'cg41', 'cg42', 'cg43', 'cg44', 'cg45', 'cg46', 'cg47', 'cg48', 'cg49', 'cg50', 'cg51', 'cg52', 'cg53', 'cg54', 'cg55', 'cg56', 'cg57', 'cg58', 'cg59', 'cg60', 'cg61', 'cg62', 'cg63', 'cg64', 'cg65', 'cg66', 'cg67', 'cg68', 'cg69', 'cg70', 'cg71', 'cg72', 'cg73', 'cg74', 'cg75', 'ch01', 'ch02', 'ch03', 'ch04', 'ch05', 'ch06', 'ch07', 'ch08', 'ch09', 'ch10', 'ch11', 'ch12', 'ch13', 'ch14', 'ch15', 'ch16', 'ch17', 'ch18', 'ch19', 'ch20', 'ch21', 'ch22', 'ch23', 'ch24', 'ch25', 'ch26', 'ch27', 'ch28', 'ch29', 'ch30', 'cj01', 'cj02', 'cj03', 'cj04', 'cj05', 'cj06', 'cj07', 'cj08', 'cj09', 'cj10', 'cj11', 'cj12', 'cj13', 'cj14', 'cj15', 'cj16', 'cj17', 'cj18', 'cj19', 'cj20', 'cj21', 'cj22', 'cj23', 'cj24', 'cj25', 'cj26', 'cj27', 'cj28', 'cj29', 'cj30', 'cj31', 'cj32', 'cj33', 'cj34', 'cj35', 'cj36', 'cj37', 'cj38', 'cj39', 'cj40', 'cj41', 'cj42', 'cj43', 'cj44', 'cj45', 'cj46', 'cj47', 'cj48', 'cj49', 'cj50', 'cj51', 'cj52', 'cj53', 'cj54', 'cj55', 'cj56', 'cj57', 'cj58', 'cj59', 'cj60', 'cj61', 'cj62', 'cj63', 'cj64', 'cj65', 'cj66', 'cj67', 'cj68', 'cj69', 'cj70', 'cj71', 'cj72', 'cj73', 'cj74', 'cj75', 'cj76', 'cj77', 'cj78', 'cj79', 'cj80', 'ck01', 'ck02', 'ck03', 'ck04', 'ck05', 'ck06', 'ck07', 'ck08', 'ck09', 'ck10', 'ck11', 'ck12', 'ck13', 'ck14', 'ck15', 'ck16', 'ck17', 'ck18', 'ck19', 'ck20', 'ck21', 'ck22', 'ck23', 'ck24', 'ck25', 'ck26', 'ck27', 'ck28', 'ck29', 'cl01', 'cl02', 'cl03', 'cl04', 'cl05', 'cl06', 'cl07', 'cl08', 'cl09', 'cl10', 'cl11', 'cl12', 'cl13', 'cl14', 'cl15', 'cl16', 'cl17', 'cl18', 'cl19', 'cl20', 'cl21', 'cl22', 'cl23', 'cl24', 'cm01', 'cm02', 'cm03', 'cm04', 'cm05', 'cm06', 'cn01', 'cn02', 'cn03', 'cn04', 'cn05', 'cn06', 'cn07', 'cn08', 'cn09', 'cn10', 'cn11', 'cn12', 'cn13', 'cn14', 'cn15', 'cn16', 'cn17', 'cn18', 'cn19', 'cn20', 'cn21', 'cn22', 'cn23', 'cn24', 'cn25', 'cn26', 'cn27', 'cn28', 'cn29', 'cp01', 'cp02', 'cp03', 'cp04', 'cp05', 'cp06', 'cp07', 'cp08', 'cp09', 'cp10', 'cp11', 'cp12', 'cp13', 'cp14', 'cp15', 'cp16', 'cp17', 'cp18', 'cp19', 'cp20', 'cp21', 'cp22', 'cp23', 'cp24', 'cp25', 'cp26', 'cp27', 'cp28', 'cp29', 'cr01', 'cr02', 'cr03', 'cr04', 'cr05', 'cr06', 'cr07', 'cr08', 'cr09']\n",
      "\n",
      "Categories in Brown Corpus:\n",
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "\n",
      "File IDs in 'news' category:\n",
      "['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', 'ca18', 'ca19', 'ca20', 'ca21', 'ca22', 'ca23', 'ca24', 'ca25', 'ca26', 'ca27', 'ca28', 'ca29', 'ca30', 'ca31', 'ca32', 'ca33', 'ca34', 'ca35', 'ca36', 'ca37', 'ca38', 'ca39', 'ca40', 'ca41', 'ca42', 'ca43', 'ca44']\n",
      "\n",
      "Categories for first 5 files:\n",
      "['news']\n",
      "\n",
      "Raw content sample:\n",
      "\n",
      "\n",
      "\tThe/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn of/in Atlanta's/np$ recent/jj primary/nn election/nn produced/vbd ``/`` no/at evidence/nn ''/'' that/cs any/dti irregularities/nns took/vbd place/nn ./.\n",
      "\n",
      "\n",
      "\tThe/at jury/nn further/rbr said/vbd in/in term-end/nn presentments/nns that/cs the/at City/nn-tl Executive/jj-tl Committee/nn-tl ,/, which/wdt had/hvd over-all/jj charge/nn of/in the/at election/nn ,/, ``/`` deserves/vbz the/at praise/nn and/c\n",
      "\n",
      "Raw content of files 'ca01' and 'ca02':\n",
      "\n",
      "\n",
      "\tThe/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn of/in Atlanta's/np$ recent/jj primary/nn election/nn produced/vbd ``/`` no/at evidence/nn ''/'' that/cs any/dti irregularities/nns took/vbd place/nn ./.\n",
      "\n",
      "\n",
      "\tThe/at jury/nn further/rbr said/vbd in/in term-end/nn presentments/nns that/cs the/at City/nn-tl Executive/jj-tl Committee/nn-tl ,/, which/wdt had/hvd over-all/jj charge/nn of/in the/at election/nn ,/, ``/`` deserves/vbz the/at praise/nn and/c\n"
     ]
    }
   ],
   "source": [
    "# 1. Get all file IDs in the corpus\n",
    "file_ids = brown.fileids()\n",
    "print(\"\\nFile IDs:\")\n",
    "print(file_ids)\n",
    "\n",
    "# 2. Get file IDs corresponding to specific categories\n",
    "categories = brown.categories()\n",
    "print(\"\\nCategories in Brown Corpus:\")\n",
    "print(categories)\n",
    "\n",
    "file_ids_in_news = brown.fileids(categories=['news'])\n",
    "print(\"\\nFile IDs in 'news' category:\")\n",
    "print(file_ids_in_news)\n",
    "\n",
    "# 3. Get categories corresponding to specific file IDs\n",
    "categories_for_files = brown.categories(fileids=file_ids[:5])\n",
    "print(\"\\nCategories for first 5 files:\")\n",
    "print(categories_for_files)\n",
    "\n",
    "# 4. Get raw content of the whole corpus\n",
    "raw_content = brown.raw()\n",
    "print(\"\\nRaw content sample:\")\n",
    "print(raw_content[:500])  # Print first 500 characters\n",
    "\n",
    "# 5. Get raw content of specific files\n",
    "raw_content_files = brown.raw(fileids=['ca01', 'ca02'])\n",
    "print(\"\\nRaw content of files 'ca01' and 'ca02':\")\n",
    "print(raw_content_files[:500])  # Print first 500 characters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw content of 'news' and 'editorial' categories:\n",
      "\n",
      "\n",
      "\tThe/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn of/in Atlanta's/np$ recent/jj primary/nn election/nn produced/vbd ``/`` no/at evidence/nn ''/'' that/cs any/dti irregularities/nns took/vbd place/nn ./.\n",
      "\n",
      "\n",
      "\tThe/at jury/nn further/rbr said/vbd in/in term-end/nn presentments/nns that/cs the/at City/nn-tl Executive/jj-tl Committee/nn-tl ,/, which/wdt had/hvd over-all/jj charge/nn of/in the/at election/nn ,/, ``/`` deserves/vbz the/at praise/nn and/c\n",
      "\n",
      "Total words in the corpus:\n",
      "1161192\n",
      "\n",
      "Words in files 'ca01' and 'ca02':\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.', 'The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise']\n",
      "\n",
      "Words in 'news' and 'editorial' categories:\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.', 'The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise']\n",
      "\n",
      "Total sentences in the corpus:\n",
      "57340\n",
      "\n",
      "Sentences in files 'ca01' and 'ca02':\n",
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']]\n",
      "\n",
      "Sentences in 'news' and 'editorial' categories:\n",
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 6. Get raw content of specific categories\n",
    "raw_content_categories = brown.raw(categories=['news', 'editorial'])\n",
    "print(\"\\nRaw content of 'news' and 'editorial' categories:\")\n",
    "print(raw_content_categories[:500])  # Print first 500 characters\n",
    "\n",
    "# 7. Get all words in the corpus\n",
    "words_in_corpus = brown.words()\n",
    "print(\"\\nTotal words in the corpus:\")\n",
    "print(len(words_in_corpus))\n",
    "\n",
    "# 8. Get words from specific file IDs\n",
    "words_in_files = brown.words(fileids=['ca01', 'ca02'])\n",
    "print(\"\\nWords in files 'ca01' and 'ca02':\")\n",
    "print(words_in_files[:50])  # Print first 50 words\n",
    "\n",
    "# 9. Get words from specific categories\n",
    "words_in_categories = brown.words(categories=['news', 'editorial'])\n",
    "print(\"\\nWords in 'news' and 'editorial' categories:\")\n",
    "print(words_in_categories[:50])  # Print first 50 words\n",
    "\n",
    "# 10. Get all sentences in the corpus\n",
    "sentences_in_corpus = brown.sents()\n",
    "print(\"\\nTotal sentences in the corpus:\")\n",
    "print(len(sentences_in_corpus))\n",
    "\n",
    "# 11. Get sentences from specific file IDs\n",
    "sentences_in_files = brown.sents(fileids=['ca01', 'ca02'])\n",
    "print(\"\\nSentences in files 'ca01' and 'ca02':\")\n",
    "print(sentences_in_files[:3])  # Print first 3 sentences\n",
    "\n",
    "# 12. Get sentences from specific categories\n",
    "sentences_in_categories = brown.sents(categories=['news', 'editorial'])\n",
    "print(\"\\nSentences in 'news' and 'editorial' categories:\")\n",
    "print(sentences_in_categories[:3])  # Print first 3 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences from files 'ca01' and 'ca02':\n",
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 13. Get sentences from specific file IDs\n",
    "sentences_files_specific = brown.sents(fileids=['ca01', 'ca02'])\n",
    "print(\"\\nSentences from files 'ca01' and 'ca02':\")\n",
    "print(sentences_files_specific[:3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences from 'news' and 'hobbies' categories:\n",
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 14. Get sentences from specific categories\n",
    "sentences_categories_specific = brown.sents(categories=['news', 'hobbies'])\n",
    "print(\"\\nSentences from 'news' and 'hobbies' categories:\")\n",
    "print(sentences_categories_specific[:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Absolute path of file 'ca01':\n",
      "C:\\Users\\Praveena\\AppData\\Roaming\\nltk_data\\corpora\\brown\\ca01\n"
     ]
    }
   ],
   "source": [
    "# 15. Get the absolute path of a file\n",
    "file_path = brown.abspath('ca01')\n",
    "print(\"\\nAbsolute path of file 'ca01':\")\n",
    "print(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoding of file 'ca01':\n",
      "ascii\n"
     ]
    }
   ],
   "source": [
    "# 16. Get the encoding of a file\n",
    "file_encoding = brown.encoding('ca01')\n",
    "print(\"\\nEncoding of file 'ca01':\")\n",
    "print(file_encoding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Opened file stream for 'ca01':\n",
      "\n",
      "\n",
      "\tThe/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn of/in Atlanta's/np$ recent/jj primary/nn election/nn produced/vbd ``/`` no/at evidence/nn ''/'' that/cs any/dti irregularities/nns took/vbd place/nn ./.\n",
      "\n",
      "\n",
      "\tThe/at jury/nn further/rbr said/vbd in/in term-end/nn presentments/nns that/cs the/at City/nn-tl Executive/jj-tl Committee/nn-tl ,/, which/wdt had/hvd over-all/jj charge/nn of/in the/at election/nn ,/, ``/`` deserves/vbz the/at praise/nn and/c\n"
     ]
    }
   ],
   "source": [
    "# 17. Open a stream for reading a corpus file\n",
    "file_stream = brown.open('ca01')\n",
    "print(\"\\nOpened file stream for 'ca01':\")\n",
    "print(file_stream.read(500))  # Read first 500 characters\n",
    "file_stream.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Root path of the Brown Corpus:\n",
      "C:\\Users\\Praveena\\AppData\\Roaming\\nltk_data\\corpora\\brown\n"
     ]
    }
   ],
   "source": [
    "# 18. Get the root path of the corpus\n",
    "root_path = brown.root\n",
    "print(\"\\nRoot path of the Brown Corpus:\")\n",
    "print(root_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contents of README file:\n",
      "BROWN CORPUS\n",
      "\n",
      "A Standard Corpus of Present-Day Edited American\n",
      "English, for use with Digital Computers.\n",
      "\n",
      "by W. N. Francis and H. Kucera (1964)\n",
      "Department of Linguistics, Brown University\n",
      "Providence, Rhode Island, USA\n",
      "\n",
      "Revised 1971, Revised and Amplified 1979\n",
      "\n",
      "http://www.hit.uib.no/icame/brown/bcm.html\n",
      "\n",
      "Distributed with the permission of the copyright holder,\n",
      "redistribution permitted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 19. Get the contents of the README file of the corpus\n",
    "readme_content = brown.readme()\n",
    "print(\"\\nContents of README file:\")\n",
    "print(readme_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
